{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "df = spark.sql('''select 'spark' as hello''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = spark.sparkContext.wholeTextFiles(\"hdfs://localhost:9000/data/mini_newsgroups/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('filename', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "])\n",
    "\n",
    "texts_df = spark.createDataFrame(texts, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9156\\1772905776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "texts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9156\\1107715065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "texts_df.show(n=5, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o128.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3262)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9156\\3310264552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \"\"\"\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o128.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3262)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "texts_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = texts_df.withColumn(\n",
    "    'newsgroup', \n",
    "    fun.split('filename', '/').getItem(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o180.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3262)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9156\\3310264552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \"\"\"\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.8-bin-without-hadoop\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o180.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3262)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3260)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "texts_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_counts = texts_df.groupBy('newsgroup').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newsgroup_counts.plot(kind='bar', figsize=(10, 5))\n",
    "plt.xticks(\n",
    "    ticks=range(len(newsgroup_counts)), \n",
    "    labels=newsgroup_counts['newsgroup']\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.title('20 news groups')\n",
    "plt.savefig('newsgroup.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df = texts_df.filter(texts_df.newsgroup == \"alt.atheism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|            filename|                text|  newsgroup|\n",
      "+--------------------+--------------------+-----------+\n",
      "|hdfs://localhost:...|Xref: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Xref: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "|hdfs://localhost:...|Path: cantaloupe....|alt.atheism|\n",
      "+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sel_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Xref: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Xref: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "|Path: cantaloupe....|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sel_df1 = sel_df.drop(\"filename\",\"newsgroup\")\n",
    "sel_df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', \"a's\", 'able', 'about', 'above']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df = spark.read.text(\"file:///D:/Big-Data-Science/Big-Data-Analysis-with-Python-master/Lesson03/data/Stopwordlist.txt\").rdd\n",
    "lines = rdd_df.map(lambda line: line[0])\n",
    "stop_list = lines.collect()[1:-1]\n",
    "stop_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '1080', '&c', '10-point', '10th']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df = spark.read.text(\"file:///D:/Big-Data-Science/Big-Data-Analysis-with-Python-master/Lesson03/data/english_words.txt\").rdd\n",
    "lines = rdd_df.map(lambda line: line[0])\n",
    "words_list = lines.collect()\n",
    "words_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "# text_list = sel_df1.select(\"text\").map(r => r.getString(0)).collect().toList\n",
    "# test_list[0]\n",
    "distinct_words = []\n",
    "sel_dfc = sel_df1.collect()\n",
    "for row in sel_dfc:\n",
    "    row_list = row['text'].split(' ')\n",
    "    #print(row_list[0],len(row_list))\n",
    "    inter_list = [token for token in row_list if token.strip() in words_list]\n",
    "    diff_list = [token for token in inter_list if token.lower().strip() not in stop_list]\n",
    "    distinct_words += diff_list\n",
    "cnt_dict = collections.Counter(distinct_words)\n",
    "cnt_dict['hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('article', 102), ('one', 70), ('people', 55), ('make', 37), ('moral', 32), ('claim', 27), ('Christian', 27), ('University', 24), ('morality', 24), ('objective', 23), ('fact', 23), ('evidence', 21), ('God', 21), ('find', 20), ('theism', 20), ('things', 16), ('observations', 16), ('made', 15), ('makes', 15), ('atheists', 15)]\n"
     ]
    }
   ],
   "source": [
    "sort_words = {token: cnt for token, cnt in sorted(cnt_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(list(sort_words.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAEWCAYAAAAD/hLkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsvUlEQVR4nO3debhdZX328e9twDCHURtRjEAAASFKQEYbMKVWKUOLglAxYE1RBIGiovIqWrXYYlFAwYDIKGCYpZShMsUwJYGQgVEZCogIMoYwJvf7x3oO2Tk5Y7L3OWfvfX+u61xn7bWetdazzsb8XNP9yDYRERHN6m2D3YGIiIhlkUIWERFNLYUsIiKaWgpZREQ0tRSyiIhoailkERHR1FLIIoY4SRMk/a6B2x8n6fGlXHcnSffXu08R/ZFCFtFkJFnShoO1fi3bU2xvXI9tRSytFLKIiGhqKWQRQ4SkoyX9QdJLku6RtFcXbW4uk3dLmidpny7abCPpVknPS3pS0smS3t7b+pL+VdKfyzoH1swfLul4Sf8n6SlJp0pasSxb7LKkpK9JeqIcw/2SPlrmHytpsqRzy7LZkjaS9PWyz8ck7VqPv2O0nxSyiKHjD8BOwAjgO8C5kkbWNrD9kTK5pe1VbF/YxXYWAEcAawPbAR8FvtjL+n9V9rsu8Dngp5LWKMuOAzYCxgAbljbf6rxTSRsDXwK2tr0q8LfAIzVN/h44B1gDuAu4hurfoHWB7wI/7/5PE9G9FLKIIcL2ZNt/tL2wFJgHgW2WYjszbN9m+03bj1AViL/uZbU3gO/afsP2VcA8YGNJAiYCR9h+1vZLwA+AfbvYxgJgOLCppOVtP2L7DzXLp9i+xvabwGRgHeA4228AFwCjJK3e3+ONSCGLGCIkHSBpZrkk+DywOdVZVX+3s5GkKyX9SdKLVIWnt+38pRSYDvOBVaiKzUrAjJp+XV3mL8b274HDgWOBP0u6QNK7apo8VTP9CvCM7QU1nyn7jOiXFLKIIUDSe4HTqC7NrWV7dWAOoKXY3CnAfcBo26sB31jK7QA8Q1VkNrO9evkZYbvLgmP7V7Z3BN4LGPjhUu43os9SyCKGhpWp/uF/GqA8bLF5N22fAtbvYVurAi8C8yRtAnyhn+u/xfZCqgJ7gqR3lL6tK+lvO7eVtLGkXSQNB16lKoAL+7KfiGWRQhYxBNi+B/gRcCtVofkAMLWb5scCZ5VLfZ+StF55AnG9svwoYD/gJaoi1PmBkMXW70P3vgb8HritXKr8X6Crd8eGUz0Y8gzwJ+AdwNf7sP2IZaIMrBkREc0sZ2QREdHUUsgiIqKppZBFRERTSyGLiIimttxgd6DdrL322h41atRgdyMioqnMmDHjGdtLvIgPKWQDbtSoUUyfPn2wuxER0VQkPdrdslxajIiIppZCFhERTS2FLCIimloKWURENLUUsoiIaGopZBER0dRSyCIioqmlkEVERFPLC9EDbPYTLzDq6P8e7G4slUeO+8RgdyEiYgktd0Ym6Rs106Mkzenn+gdLOqCH5eMkbb8sfYyIiPppuUIGfKP3Jt2zfarts3toMg5IIYuIGCKaupBJukzSDElzJU2UdBywoqSZks4rzYZJOq20uVbSimXdDSRdXdafImmTMv9YSUeV6cMk3SNplqQLJI0CDgaOKPvYSdInJc2RdLekmwfhzxAR0daa/R7ZQbafLcVpGvDXwJdsj4Hq0iIwGvi07c9L+jXwj8C5wCTgYNsPSvow8DNgl07bPxp4n+3XJK1u+3lJpwLzbB9f9jEb+FvbT0havatOSpoITAQYtlqX4c0REbGUmr2QHSZprzL9Hqqi1dnDtmeW6RnAKEmrUF0enCypo93wLtadBZwn6TLgsm76MBU4sxTJS7pqYHsSVeFk+MjR7uF4IiKin5q2kEkaB4wHtrM9X9KNwApdNH2tZnoBsCLVJdXnO87cevAJ4CPA3wPflPSBzg1sH1zO6D4BzJC0le2/9O9oIiJiaTXzPbIRwHOliG0CbFvmvyFp+Z5WtP0i8LCkTwKosmVtG0lvA95j+wbga2V/qwAvAavWtNvA9u22vwU8TXVmGBERA6Rpz8iAq4GDJd0L3A/cVuZPAmZJuhP4Zg/r7w+cIukYYHngAuDumuXDgHMljQAEnFjukf0GuEjSHsChVA9+jC5tfttpG0v4wLojmJ73sSIi6kZ2btkMpLFjxzojREdE9I+kGbbHdrWsmc/ImlKSPSIi6quZ75E1jKQJkk4e7H5ERETvUsgiIqKptWwhKzmL90k6U9IDks6TNF7SVEkPStqm/Nwq6S5Jt0jauIvtfKK0WVvSrmX6TkmTy/toSDquJgHk+IE/2oiI9tWyhazYEPgRsEn52Q/YETiKKpPxPmAn2x8EvgX8oHbl8rL10cDHy6xjgPG2PwRMB46UtBawF7CZ7S2A73XuRInPmi5p+oL5L9T/KCMi2lirP+zxsO3ZAJLmAr+17RIrNYrq3bCzyuPzpnoMv8MuwFhgV9svStoN2BSYWtJA3g7cCrwAvAr8QtKVwJWdO5Fkj4iIxmn1M7LaVI+FNZ8XUhXxfwNusL05VXpHbTLIH6hefN6ofBZwne0x5WdT25+z/SawDXARsBvV+20RETFAWr2Q9WYE8ESZntBp2aNUAcNnS9qM6oXrHSRtCCBpZUkblftkI2xfBRwBbElERAyYVr+02Jv/oLq0eAywxMtdtu+TtD8wmeqMbQJwvqSOgOFjqCKrLpe0AtVZ25E97TDJHhER9ZVkjwGWZI+IiP5LsscQkmSPiIj6avd7ZEtF0o2Suvx/BhERMbBSyCIioqm1TSFblqQPSStKukDSvZIupRqcs2O7XaZ9RETEwGibQlYsbdLHF4D5tt8PfBvYCkDS2nSR9tF5p0n2iIhonHZ72GNpkz4+ApwIYHuWpFll/rZ0nfaxmCR7REQ0TrsVsr4mfewlaRRwYy/b60j7+HSd+xkREX3UbpcWe9Nd0sfNVJchkbQ5sEWZ32Xax8B0NSIioP3OyHrTXdLHKcAvJd0L3AvMALD9tKQJLJn28UB3O0iyR0REfSXZY4Al2SMiov+S7DGENHOyR62kfETEUDGg98jKu1xzBnKf3fRjjKSP13zeXdLRg9mniIhYOk3/sIekpTmrHMOiUZ+xfYXt4+rWqYiIGDANLWSSjpQ0p/wcXmYvV1I17pV0kaSVStvjJN0jaZak48u8dSRdLGla+dmhzD9W0jmSpgLnSLqtjBnWsd8bJY3tKqlD0tuB7wL7SJopaR9JEySdXNYdJen60o/fSlqvzD9T0ollOw9J2rvMHynp5rKtOZJ2auTfNCIiFtewQiZpK+BA4MNULw5/HlgD2Bj4WUnJeBH4oqS1gL2AzWxvAXyvbOYnwAm2t6Ya5PL0ml1sSpWo8WngQuBTZb8jgZG2p9NFUoft18v0hWWk5ws7df0k4KzSj/MoL0IXI6mSQHYDOs7g9gOusT2GalDNmV38LZLsERHRII08I9sRuNT2y7bnAZcAOwGP2Z5a2pxb2r0AvAr8QtI/APPL8vHAyZJmAlcAq9VkGV5h+5Uy/Wtg7zL9KeCiMj0CmFzuy50AvHXW1oPtgF+V6XNK/zpcZnuh7XuAd5Z504ADJR0LfMD2S503aHuS7bG2xw5baUQfuhAREX01GPfIOj/vb9tvAttQFaDdgKvLsrcB25YzpzG21y1FEeDlmg08AfxF0hbAPlRnaLAoqWNzqhGeV1jGvtcmg6js+2aqCKsngDMlHbCM+4iIiH5oZCGbAuwpaSVJK1NdOpwCrCdpu9JmP+B35SxrhO2rgCOoLtEBXAsc2rFBSWN62N+FwFfLdjqyELtL6ngJWLWb7dwC7Fum9y997pak9wJP2T6N6tLnh3pqHxER9dWw98hs3ynpTOCOMut04DngfuAQSWcA91ClZowALpe0AtWZTkeC/GHAT0tI73JUUVEHd7PLi6juqf1bzbzukjpuAI4ulyz/vdN2DqVK8fgK8DTVfb6ejAO+IukNYB7Q4xlZkj0iIuoryR4DLMkeERH9l2SPISTJHhER9dX0L0R3R9Lqkr5YpsdJurKbdqdL2nRgexcREfXSsoUMWB34Ym+NbP9zeZw+IiKaUCsXsuOADcoDHf8JrFKSRO4rySKCRSkgZXqepO9LurukhbyzzN+gfJ4t6XuS5pX5SfWIiBhkrVzIjgb+UBI3vgJ8EDicKhFkfWCHLtZZGbjN9pZUT0h+vsz/CfAT2x8AHq9p32uqByTZIyKikVq5kHV2h+3HbS+kKjijumjzOtBxL21GTZvtgMll+lc17XtN9YAke0RENFI7FbLaVI4FdP3E5hte9D5Cd23eklSPiIjB18qFrKf0jv66jSq0GBalfiTVIyJiCGjZ98hs/0XS1BIY/Arw1DJs7nDgXEnfpMqB7LjRNY5+pHpAkj0iIuotyR59UMZMe8W2Je0LfNr2HkuzrSR7RET0X5I9lt1WVMPJCHgeOGhpN9QqyR5LI2kgEdEILXWPrK9pHj2s/11J4zvPtz3F9pa2t7D9Edu/r1efIyJi2bRUIaOPaR7dsf0t2/9bv+5ERESjtVoh62uax1aSbpI0Q9I1kkaW+WdK2rtMHyfpHkmzJB1fs/yUkvLxUDnrO0PSvWXImoiIGGCtdo/saGBz22MkjQMuBzYD/ghMBXaQdDtwErCH7acl7QN8n5r7XpLWohoIdJPygMfqNftYg+oF6d2BK6gSQv4ZmCZpjO2ZnTslaSIwEWDYauvU83gjItpeqxWyzu6w/ThAOUsbRfWwxubAdeUEbRjwZKf1XgBeBX5R7rPV3mv7TSlus6neIZtdtj+3bH9m507YngRMAhg+cnQeE42IqKNWL2RdpXkImGt7u+5Wsv2mpG2AjwJ7A18Cdum0zYWdtr+Q1v97RkQMOa12j6wvaR73A+tI2g5A0vKSNqttIGkVYITtq4AjqAKBIyJiCGqpM4i+pHnYfr080HGipBFUf4MfA3Nrmq0KXC5pBaozuCPr1ccke0RE1FeSPQZYkj0iIvovyR5DSJI9IiLqq9XukS0TSYeVd8Kek3R0P9d96x20iIgYODkjW9wXgfEdj+xHRMTQl0JWSDoVWB/4H0lnABvY/lJJ7HgRGAv8FfBV2xeVlJCTgL8BHqMaXToiIgZYLi0Wtg+mSgDZGXiu0+KRwI7AblQxWFAlf2wMbEo1Dtn23W1b0kRJ0yVNXzD/he6aRUTEUkgh65vLbC+0fQ/wzjLvI8D5thfY/iNwfXcr255ke6ztscNWGjEQ/Y2IaBspZH1Tm+ChQetFREQsIYVs6d0M7CNpWEnP33mwOxQR0Y7ysMfSu5Qqf/Ee4P+AW/uyUpI9IiLqK8keAyzJHhER/ZdkjyGknZM9aiXlIyLqJffIakh6p6RfldGfZ0i6VdJe/Vj/Rkld/j+GiIhojBSyorzgfBlws+31bW8F7Au8e1A7FhERPUohW2QX4HXbp3bMsP2o7ZMkrSDpl5JmS7pL0s4AklaUdEHJZ7wUWHGwOh8R0a5yj2yRzYA7u1l2CGDbH5C0CXCtpI2ALwDzbb9f0hbdrS9pIjARYNhq69S/5xERbSxnZN2Q9FNJd0uaRhVPdS6A7fuAR4GNqNI9OubPAmZ1ta0ke0RENE4K2SJzgQ91fLB9CPBRIKdQERFDWArZItcDK0j6Qs28lcrvKcD+AOWS4nrA/VTpHvuV+ZsDWwxYbyMiAsg9srfYtqQ9gRMkfRV4GngZ+BpwOXCKpNnAm8AE269JOgX4paR7gXuBGb3tJ8keERH1lUJWw/aTVI/cd+XALtq/0kP7iIgYAClkAyzJHpUke0REvbTcPTJJt3Qz/0xJew90fyIiorFarpDZ7nak5oiIaD1NUcgk/ZOkOyTNlPRzSYdI+s+a5RMknVym55XfknSypPsl/S/wjpr2W0m6qeQpXlPGE+vISvxh2dcDknYq84dJOl7SHEmzJB3a03YiImLgDPlCJun9wD7ADrbHAAuAeUBtmO8+wAWdVt0L2BjYFDgA2L5sb3ngJGDvkqd4BvD9mvWWs70NcDjw7TJvIjAKGGN7C+C8Pmyn9hgmSpouafqC+S/0908QERE9aIaHPT4KbAVMq3J9WRH4M/CQpG2BB4FNgKmd1vsIcL7tBcAfJV1f5m8MbA5cV7Y3DHiyZr1Lyu8ZVMULYDxwqu03AWw/W94b62k7b7E9CZgEMHzk6AwAFxFRR30qZJJWBl6xvbC8ELwJ8D+232ho78rugbNsf71Tnw4CPgXcB1zqvo8QKmCu7e26Wf5a+b2Anv8+vW0nIiIGQF8vLd5MlXqxLnAt8BngzEZ1qpPfAntLegeApDUlvRe4FNgD+DRLXlaEqs/7lPtbI4Gdy/z7gXUkbVe2t7ykzXrpw3XAv0harqMPS7mdiIios75eWpTt+ZI+B/zM9n9ImtnAfr3F9j2SjqFKnH8b8AZwiO1HS6LGprbv6GLVS6mGZrkH+D/g1rK918tj+CdKGkH1N/gxVdZid06nCgmeJekN4DTbJy/FdpLsERFRZ+rLFTlJdwFfBE4APmd7rqTZtj/Q6A62mrFjx3r69OmD3Y2IiKYiaYbtsV0t6+sZ2eHA16nuRc2VtD5wQ53611aS7NGzJH5ERH/16R6Z7Zts7277h+XzQ7YPa2zXBp6kw8poz+f1c73DJa3Ue8uIiKi3Hs/IJP0G6Pbao+3d696jwfVFYLztx/u53uFUA2zOr3uPIiKiR71dWjy+/P4H4K8ooyFTPSn4VKM6NRgknQqsD/yPpHOBPYEVgFeAA23fL2kY8EPgY8BC4DSqx/DfBdwg6RnbO3e1/YiIaIweC5ntmwAk/ajTTbbfSGqpJxZsHyzpY1SP6b8O/Mj2m5LGAz8A/pHFEz7elLRmeTn6SGBn2890tW1JE8u6DFstA05HRNRTXx/2WFnS+rYfApD0PmDlxnVr0I0AzpI0murS6vJl/hIJH33ZWJI9IiIapz9PLd4o6SGqS2nvpZxhtKh/A26wvZekUcCNg9udiIjoTq+FrLyEPAIYTRVNBXCf7de6X6vpjQCeKNMTauZ3JHzcUHtpEXgJWBXo8tJiREQ0Tq+FrOQrftX2r4G7B6BPQ8F/UF1aPAaofelriYQP4GSqy4ZXS/pjbw97JNkjIqK++prscRzV2caFwMsd8/t6jygWSbJHRET/9ZTs0ddC9nAXs217/WXtXLsZPnK0R372x4PdjSEryR4R0ZWeCllfkz3e18XPoBUxSaMkzelm2emSNl2KbY6R9PGaz7tLOnpZ+hkREY3X1/HIlge+QDVYJVRP8f18gMYj6xfb/7yUq44BxgJXle1cAVxRp25FRESD9HU8slOoRmn+WfnZqswbEJKOlDSn/BxeZi8n6bySjXhRR9ahpBsljS3Tu0q6VdKdkiZLWqXM31rSLZLulnRHGYblu1Tjl82UtI+kCZJOljRC0qPl6U0krSzpsTL+2AaSrpY0Q9IUSZss2fuIiGikvhayrW1/1vb15edAYOtGdqyDpK2AA4EPA9sCnwfWADamGhvt/cCLVDmJteutDRxDlZ34IWA6cKSkt1M9tPJl21tSveT8MvAt4ELbY2xf2LEd2y8AM4G/LrN2A64pZ6OTgENtbwUcRVXkuzqGiZKmS5q+YP4Ly/oniYiIGn0tZAskbdDxoQzjsqAxXVrCjlTDx7xsex5wCbAT8JjtqaXNuaVdrW2BTYGpZRDQz1K9yL0x8KTtaQC2X+xI6ujBhcA+ZXpf4MJydrc9MLls/+fAyK5Wtj3J9ljbY4etNKKPhx0REX3RW/r94cAtwNHA9TVPL44CDmpoz3rX+XHLzp8FXGf704vNlJZmMNArgB9IWpPqsur1VBFdz9sesxTbi4iIOuntjOzdwI+pzkieAp4FLga2t319Y7v2linAnpJWkrQysFeZt56k7Uqb/YDfdVrvNmAHSRvCW/e2NgLuB0ZK2rrMX1XScixK51hCOROcBvwEuNL2AtsvAg9L+mTZjiRtWb/DjoiIvugt/f4ogHJfaSzVpbRxwNclPW+734+595ftOyWdCdxRZp0OPEdVkA6RdAZwD4s/fGLbT0uaAJwvaXiZf4ztByTtA5wkaUWqYVrGU414fXS5TPjvXXTlQmAy1fF32B84pSSALA9cQC/pJ0n2iIior76+ED0C2A7YofxeHZhdHvoYUiTNBna33dVL3IMuyR4REf3X0wvRvd0jmwRsRnXZ7Xaq+2X/Zfu5uveyDiRdR1Vgh2QRA5j9xAuMOvq/e28YSfmIiD7p7R7ZesBw4E9UafCPA883uE9Lzfbf2N6vHtuqTQ+RNFbSiWV6nKTt67GPiIhYdr3dI/uYJFGdlW0P/CuwuaRngVttf3sA+tgwkpbrw6P32J5O9R4aVPfI5lGdnUZExCDr9T0yV+ZQRTf9DzAV2AD4coP71q1ytnSfpDMlPVASPsZLmirpQUnbSFpT0mWSZkm6TdIWZd1jJZ0jaSpwTtnWlJL+cWdXZ1vlLOxKVYNsHgwcURJAdpL0cInwQtJqtZ8jIqLxertHdhjVmdj2wBtUZyG3AGcAsxveu55tCHyS6n22aVSP4O8I7A58A3gMuMv2npJ2Ac6mylOE6kXpHW2/UqKt/sb2q5JGA+dTPaG5BNuPSDoVmGf7eKgisYBPAJdRvSx9SecMSkkTKSNqD1ttnbocfEREVHoLDR5F9cj5EbafbHx3+uVh27MBJM0Ffmvb5anFUVQpHv8IYPt6SWtJWq2se4XtV8r08sDJksZQpZVs1M9+nA58laqQHUgVobUY25Oo4qwYPnJ074+JRkREn/V2j+zIgerIUnitZnphzeeFVMfVUzL/yzXTR1C97L0l1aXWV/vTCdtTy+XJccCwchk2IiIGSF+zFpvRFKoXlilF5pmSxtHZCKrsxYXAZ4BhvWy3qwSQs4FfAb9chv5GRMRS6NN4ZE3qWOAMSbOA+VShwV35GXCxpAOAq1n8bK0rvwEukrQHVfL9FOA84HtU99d6lGSPiIj66lOyR/RM0t7AHrY/01vbJHtERPTfUid7RO8knQT8HfDxvrRPskffJdkjIvpiyN0jq03UqJl3rKSjeljnreSNBvbrlpr+vZUeYvtQ2xvafqCR+4+IiK4NuUK2NGxPt33Ysm6nDOfS3T46XpQeRfXOWkREDAFNVcgk3Sjph5LuKIkeO5X5Hckbb5P0iKTVa9Z5UNI7Ja0j6WJJ08rPDmV556SPzcr2Z5ZUkNGl3byyyeOAncryIyTdXN5B69jf7zIuWUTEwGnGe2TL2d5G0seBb1ONJQaA7YWSLqcafPOXkj4MPGr7KUm/Ak6w/TtJ6wHXAO8vq9YmfZwE/MT2eWUcts6P4x8NHGV7N4CSOzkBOFzVwJ0r2F5sTLIke0RENM5QPCPr7jHKjvmXlN8zqC7zdXYhsE+Z3rd8hqrgnVwGzrwCWE3SKmVZbdLHrcA3JH0NeG/N/O5MBnYr+YoHAWcu0XF7ku2xtscOW2lEL5uLiIj+GIqF7C/AGp3mrQk8U6Y7EjwW0PUZ5a3AhpLWAfZkUeF7G7Ct7THlZ13bHZcL33p3zPavqPIaXwGuKjmN3bI9H7gO2AP4FNU7ZRERMUCGXCErxeXJjgIiaU3gY8Dv+ri+gUuB/wLutf2Xsuha4NCOdrX3tWpJWh94yPaJwOXAFp2adJXscTpwIjBtqA46GhHRqobqPbIDgJ9K+q/y+Tu2/1ANjdYnF1Il4k+omXdY2eYsquO+mWpIls4+BXxG0htUA4r+oNPyWcACSXcDZ9o+wfYMSS/Sh4iqJHtERNRXkj3qQNK7gBuBTUpmY7eS7BER0X9J9migktH4feDI3ooYJNmjEZIAEtHehtw9su5I+itJF0j6g6QZkq6SNFHSlX1c/7uSxvewfE9Jm/a1fQfbZ9t+j+3JfTuSiIiop6Y4I1N1c+xS4Czb+5Z5W1I9XdiX9YfZ/lYvzfYErgTuAehD+4iIGAKa5YxsZ+AN26d2zCgvHU8BVpF0kaT7JJ1Xih4l4eOHku4EPinpzJJSj6TjJN1TkjuOl7Q9VVH8z5LYsUGn9t8qaSBzJE2q2UeXSSMRETFwmuKMDNic6gXornwQ2Az4IzAV2IFFj+r/xfaHACR9rPxeiyr5YxPblrS67eclXQFcafui0q52Hyfb/m6Zfw6wG9W4ZNBD0kiHJHtERDROs5yR9eQO24+XBy1msnjax4VdtH8BeBX4haR/oBp0szc7S7pd0mxgF6rC2aG3pJEke0RENFCzFLK5wFbdLHutZrpz2scSoz3bfhPYBriI6szq6p52LGkFqlGk97b9AeA0YIUu9t9d0khERDRQsxSy64Hh5RIdAJK2APp9T6rkK46wfRVwBNCRVN9VYgcsKlrPlHX37u8+IyKicZriDKLcy9oL+HEJ830VeAS4bCk2typweTnTEnBkmX8BcJqkw6gpVuX+2WnAHKqkj2lLexyQZI+IiHpLsscAS7JHRET/JdljCEmyR/0l2SOivTXLPbKGKSNEH9VLm4NLFFVERAwxOSPrg9oXsSMiYmhpuzMySQeURI+7y8vNtcs+XxI87pZ0saSVyvy3ztpKmscJkqZLulfS1pIukfSgpO8NxjFFRLSztipkkjYDjgF2sb0l8OVOTS6xvXVZdi/wuW429Xq56Xgq1eCbh1Clj0woySGd9zuxFL7pC+a/UK/DiYgI2qyQUaVyTLb9DIDtZzst31zSlJLgsT+LJ3jUuqL8ng3Mtf2k7deAh4D3dG6cZI+IiMZpt0LWmzOBL5UEj++weIJHrY40j4UsniyykNx3jIgYUO1WyK6nSsJfC0DSmp2Wrwo8KWl5qjOyiIgY4trq7MH2XEnfB26StAC4iyohpMP/A24Hni6/u4qsWiZJ9oiIqK8kewywJHtERPRfkj2GkCR7DC1JBYlofu12j6xhJI2SNGew+xER0W5SyPpBUs5gIyKGmLYoZOVs6T5JZ0p6QNJ5ksZLmloSObaRtKaky0rqx21lvLOOVI9zJE0FzinbmiLpzvKz/SAfXkREW2unM4wNgU8CB1GNKbYfsCOwO/AN4DHgLtt7StoFOBsYU9bdFNjR9isltupvbL8qaTRwPtDlDcgOZUDQiQDDVlun3scVEdHW2qmQPWx7NoCkucBvy4Cds4FRwHuBfwSwfb2ktSStVta9wvYrZXp54GRJY4AFwEa97dj2JGASwPCRo/OYaEREHbVTIeucwFGbzrEc8EYP675cM30E8BSwJdWl2Vfr2MeIiOintrhH1kdTKGkeksYBz9h+sYt2I4AnbS8EPgMMG6gORkTEktrpjKw3xwJnSJoFzAc+2027nwEXl4E2r2bxs7VeJdkjIqK+kuwxwJLsERHRf0n2GEKS7NFakgwSMfhyj6yPJE2QdPJg9yMiIhaXQhYREU2trQpZHxM+tpF0q6S7JN0iaeMutvOJ0mZtSbuW6TslTZa0ymAcW0REu2qrQlZsCPwI2KT8dCR8HEWV8HEfsJPtDwLfAn5Qu7KkvYCjgY+XWccA421/CJgOHNl5h5ImSpouafqC+S805KAiItpVOz7s0VvCxwjgrBI/Zaokjw67UMVR7Wr7RUm7UcVXTZUE8Hbg1s47TLJHRETjtGMh6y3h49+AG2zvJWkUcGNN+z8A61PFUk0HBFxn+9MN7nNERHSjHS8t9mYE8ESZntBp2aNUeYxnS9oMuA3YQdKGAJJWltRr9mJERNRPO56R9eY/qC4tHgMs8cKX7fsk7Q9MBv6eqtidL2l4aXIM8EB3G0+yR0REfSXZY4Al2SMiov+S7DGEJNkjmkmSS6IZ5B7ZUkjKR0TE0JFCFhERTa0lC1lNgsd5ku6VdJGklSRtJekmSTMkXSNpZGk/RtJtkmZJulTSGmX+jZJ+ImmmpDmStuliX+tIuljStPKzw0Afb0REO2vJQlZsDPzM9vuBF4FDgJOAvW1vBZwBfL+0PRv4mu0tgNnAt2u2s5LtMcAXyzqd/QQ4wfbWVI/mn965QZI9IiIap5Uf9njM9tQyfS5V/NTmwHUlhWMY8KSkEcDqtm8qbc+ierS+w/kAtm+WtJqk1TvtZzywadkmwGqSVrE9r2NGkj0iIhqnlQtZ54LxEjDX9na1M0sh6892On9+G7Ct7Vf738WIiFhWrXxpcT1JHUVrP6oUjnU65klaXtJmtl8AnpO0U2n7GeCmmu3sU9rvCLxQ2te6Fji044OkMXU/koiI6FYrn5HdDxwi6QzgHqr7Y9cAJ5azsOWAHwNzgc8Cp0paCXgIOLBmO69KuosqPPigLvZzGPBTSbPKNm8GDu6uU0n2iIior5ZM9ihhv1fa3nwZt3MjcJTtukVxJNkjIqL/kuwxhCTZI2JgJJWkfbRkIbP9CNUTisu6nXHL3JmIiGioVn7YIyIi2kAKWSeSjiwpHnMkHV5SQu6VdJqkuZKulbRiabuBpKtLUsgUSZsMdv8jItpNClkNSVtRPbH4YWBb4PPAGsBo4Ke2NwOep0rwgOol50NLUshRwM+62W6SPSIiGqQl75Etgx2BS22/DCDpEmAn4GHbM0ubGcAoSasA2wOTa1I9htOFJHtERDROClnfvFYzvQBYkeps9vmSwxgREYMklxYXNwXYsyTlrwzsVeYtwfaLwMOSPgmgypYD19WIiICckS3G9p2SzgTuKLNOB57rYZX9gVMkHUOV/HEBcHdP+0iyR0REfbVkssdQlmSPiIj+S7LHEJJkj4hoR41MWmn7e2TlXbGVaj5f1cWYY7Xtj5V01IB0LiIietXWhUzSMOBw4K1CZvvjtp8frD5FRET/tHQhk3RZSd2YK2limTdP0o8k3Q18E3gXcIOkG8ryRyStXaYPkDRL0t2Szuli+0n2iIgYZK1+j+wg28+WSKlpki4GVgZut/2vAJIOAna2/UztipI2A44Btrf9jKQ1u9j+JOBg2w9K+jBVsscunRuVIjoRYNhq69Tx8CIiotUL2WGS9irT76GKmloAXNyHdXcBJncUONvP1i5MskdExNDQsoVM0jhgPLCd7fllkMwVgFdtL6jDLpLsERExBLTyPbIRwHOliG1CFQLclZeAVbuYfz3wSUlrAXS+tJhkj4iIoaFlz8iAq4GDJd0L3A/c1k27ScDVkv5oe+eOmbbnSvo+cJOkBcBdwIRO6ybZIyJikCXZY4Al2SMiov96SvZo5UuLERHRBlLIIiKiqaWQRUREU0shi4iIppZCFhERTS2FLCIimloKWURENLUUsoiIaGp5IXqASXqJKmmkHa0NPNNrq9bTrscN7Xvs7Xrc0Lhjf6/tLocPaeWIqqHq/u7eTm91kqa347G363FD+x57ux43DM6x59JiREQ0tRSyiIhoailkA2/SYHdgELXrsbfrcUP7Hnu7HjcMwrHnYY+IiGhqOSOLiIimlkIWERFNLYVsAEn6mKT7Jf1e0tGD3Z9GkfQeSTdIukfSXElfLvPXlHSdpAfL7zUGu6+NImmYpLskXVk+v0/S7eW7v1DS2we7j/UmaXVJF0m6T9K9krZrl+9c0hHlv/U5ks6XtEKrfueSzpD0Z0lzauZ1+T2rcmL5G8yS9KFG9CmFbIBIGgb8FPg7YFPg05I2HdxeNcybwL/a3hTYFjikHOvRwG9tjwZ+Wz63qi8D99Z8/iFwgu0NgeeAzw1KrxrrJ8DVtjcBtqQ6/pb/ziWtCxwGjLW9OTAM2JfW/c7PBD7WaV533/PfAaPLz0TglEZ0KIVs4GwD/N72Q7ZfBy4A9hjkPjWE7Sdt31mmX6L6B21dquM9qzQ7C9hzUDrYYJLeDXwCOL18FrALcFFp0nLHLmkE8BHgFwC2X7f9PG3ynVOFS6woaTlgJeBJWvQ7t30z8Gyn2d19z3sAZ7tyG7C6pJH17lMK2cBZF3is5vPjZV5LkzQK+CBwO/BO20+WRX8C3jlY/WqwHwNfBRaWz2sBz9t+s3xuxe/+fcDTwC/LJdXTJa1MG3zntp8Ajgf+j6qAvQDMoPW/81rdfc8D8u9eClk0jKRVgIuBw22/WLvM1XsfLffuh6TdgD/bnjHYfRlgywEfAk6x/UHgZTpdRmzh73wNqjOP9wHvAlZmyUtvbWMwvucUsoHzBPCems/vLvNakqTlqYrYebYvKbOf6risUH7/ebD610A7ALtLeoTq8vEuVPeOVi+XnaA1v/vHgcdt314+X0RV2NrhOx8PPGz7adtvAJdQ/XfQ6t95re6+5wH5dy+FbOBMA0aXJ5neTnUz+IpB7lNDlHtCvwDutf1fNYuuAD5bpj8LXD7QfWs021+3/W7bo6i+4+tt7w/cAOxdmrXcsdv+E/CYpI3LrI8C99AG3znVJcVtJa1U/tvvOPaW/s476e57vgI4oDy9uC3wQs0lyLpJsscAkvRxqvsnw4AzbH9/cHvUGJJ2BKYAs1l0n+gbVPfJfg2sBzwKfMp255vGLUPSOOAo27tJWp/qDG1N4C7gn2y/NojdqztJY6gecHk78BBwINX/WW7571zSd4B9qJ7YvQv4Z6p7QS33nUs6HxhHNVzLU8C3gcvo4nsuhf1kqkut84EDbU+ve59SyCIiopnl0mJERDS1FLKIiGhqKWQREdHUUsgiIqKppZBFRERTW673JhExFEhaQPVKQ4c9bT8ySN2JGDLy+H1Ek5A0z/Yq3SwT1f+eF3a1PKKV5dJiRJOSNKqMb3c2MAd4j6SvSJpWxn76Tk3bb0p6QNLvynhZR5X5N0oaW6bXLtFaHeOp/WfNtv6lzB9X1ukYd+y8UkSRtLWkWyTdLekOSatKurm8KN3Rj99J2nKg/kbRHnJpMaJ5rChpZpl+GDiCapynz9q+TdKu5fM2gIArJH2EKsB3X2AM1f/m76RKZ+/J56jihLaWNByYKunasuyDwGbAH4GpwA6S7gAuBPaxPU3SasArVFFlE4DDJW0ErGD77mX7M0QsLoUsonm8YntMx4cyRM6jZZwngF3Lz13l8ypUhW1V4FLb88t6fcn43BXYQlJHVuCIsq3XgTtsP162NRMYRTV0yZO2pwF0jHYgaTLw/yR9BTiIalDGiLpKIYtobi/XTAv4d9s/r20g6fAe1n+TRbcYVui0rUNtX9NpW+OA2rzABfTw74jt+ZKuoxrm5FPAVj30JWKp5B5ZROu4BjiojAOHpHUlvQO4GdhT0oqSVgX+vmadR1hUXPbutK0vlOF4kLRRGSizO/cDIyVtXdqvWjOEyenAicA0288t0xFGdCFnZBEtwva1kt4P3Fqev5hHlbh+p6QLgbupxomaVrPa8cCvJU0E/rtm/ulUlwzvLA9zPM2i4eu72vfrkvYBTpK0ItX9sfHAPNszJL0I/LI+RxqxuDx+H9FmJB1LVWCOH6D9vQu4EdgkrwdEI+TSYkQ0jKQDqMah+2aKWDRKzsgiIqKp5YwsIiKaWgpZREQ0tRSyiIhoailkERHR1FLIIiKiqf1/h5Lau4A/TgEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ntop = 20\n",
    "x = []\n",
    "y = []\n",
    "for key, value in sort_words.items():\n",
    "    x.append(key)\n",
    "    y.append(value)\n",
    "plt.barh(x[:ntop], y[:ntop])\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('alt.atheism')\n",
    "plt.savefig('WordsFrequency.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
